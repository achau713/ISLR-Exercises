---
title: "Chapter2-Exercises"
author: "Anthony Chau"
date: "12/8/2019"
output: 
  pdf_document:
    toc: true
---

\newpage

# Conceptual Exercises

## Question 1

For each of parts (a) through (d), indicate whether we would generally
expect the performance of a flexible statistical learning method to be
better or worse than an inflexible method. Justify your answer.

(a) The sample size n is extremely large, and the number of predictors p is            small.

(b) The number of predictors p is extremely large, and the number of observations       n is small.

(c) The relationship between the predictors and response is highly
    non-linear.

(d) The variance of the error terms, i.e. $\sigma^2$ = `Var()`, is extremely          high.


_Solution:_

(a) We would expect the performance of a flexible statistical learning method to be
better than an inflexible method because the flexible methods can model the data more accurately given the large amount of observations. Also, since the number of predictors is small, the flexible methods would not estimate a large number of parameters - increasing overall interprebility.

(b) The performance of flexible methods is worse than inflexible methods. Given the large number of predictors and low number of observations, flexible methods would most likely result in overfitting as well as have low interprebility. 

(c) The performance of flexible methods is better than inflexible methods because the flexible methods can better capture non-linear relationships over inflexible methods (such as linear regression).

(d) Flexible methods would perform better than inflexible methods because the flexible methods could include more potential variables that could be useful in predicting the response. The inclusion of those variables can offset the presence of a high variance for the error terms. 

\newpage

## Question 2

2. Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.

(a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.

(b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price
charged for the product, marketing budget, competition price, and ten other variables.

(c) We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the %
change in the US market, the % change in the British market, and the % change in the German market.



_Solution:_

(a) This scenario is a regression problem because we are predicting a quantitative
variable, CEO salary. We are mainly interested in inference - understanding the
factors that affect CEO salary. For this case, n = 500 and p = 3.

(b) This scenario is a classification problem because we are predicting a qualitative
variable - whether the product is a success or a failure. We are mainly interested
in predicting - correctly determining if a new product will be a success or a failure.
For this case, n = 20 and p = 13.

(c) This scenario is a regression problem because we are predicting a quantitative variable, USD/Euro exchange rate. We are mainly interested in prediction - what the
USD/Euro exchange rate is given some predictor variables. For this case, n = 52 (52 weeks in a year) and p = 4. 

\newpage

## Question 3

3. We now revisit the bias-variance decomposition.

(a) Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represen the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.

(b) Explain why each of the five curves has the shape displayed in part (a).


_Solution:_

(a)

(b)

\newpage

## Question 4

4. You will now think of some real-life applications for statistical learning.

(a) Describe three real-life applications in which classification might be
useful. Describe the response, as well as the predictors. Is the goal of each
application inference or prediction? Explain your answer.

(b) Describe three real-life applications in which regression might be useful.
Describe the response, as well as the predictors. Is the goal of each
application inference or prediction? Explain your answer. 

(c) Describe three real-life applications in which cluster analysis might be
useful.

_Solution:_

(a) One application could be to determine if an applicant will default on a loan. The response is Yes/No, indicating whether the applicant defaulted on a loan.
Potential explanatory variables could include credit score, age, income, 
employment status, number of dependents, etc. The goal of this application is prediction - it is important to predict if an applicant will default on a loan!  
&nbsp;    
Another application could be to determine if a person would move up in socioeconomic status. The response is Yes/No, indicating whether the applicant
moved up (Yes) or down (No) from their SES status when they were born. Potential
explantatory variables could be parent's education level, parent's employment,
household income, born in city or rural area, number of siblings, etc. The main goal of this application is inference. We would like to understand the relationship between the response and the explanatory variables.  
&nbsp;  
Yet another application is to determine if a basketball team will win the championship game. The response is Yes/No, indicating if the team will win the championship game. Potential explanatory variables could include: season record, average number of points scored during season, whether team won a championship in previous year(s), etc. This application could be both for inference and prediction. It is an interesting question to pose: what leads to basketball teams winning championship games? On the other hand, you may be betting with your buddies on the championship game, so your money is on the line!


(b)
One application of regression is to predict the yearly sales for a clothing company (response variable). Potential explanatory variables could include:
past year sales, advertising expenditure, number of physical stores, number of
unique visitors to website, etc.  
&nbsp;  
Another application of regression is to model the relationship between age of death (response) with the following explanatory variables: average minutes spent exercising per week, time spent engaging in mental activities, average number of servings of fruits and vegetables, smoker status, weight, etc. This application focuses on inferences - understanding the relationship between response and predictors.  
&nbsp;  
Another application of regression could be to model the number of delayed flights at an airport in a day(response). Potential predictors could include: average daily passengers total, location of airport, domestic/international airport status, average number of thunderstorms/severe weather incidents occured per year, number of average daily nonstop passengers, number of TSA checkpoints, etc. The main interest of this application is inference: what are the key factors which contribute to long airport delays.

(c)
One application of cluster analysis is market segmentation of customers when
a company wants to introduce a product to a new market. Cluster analysis can
help the company gain insight into which customers would be most likely to be interested in the product or buy more of the product.  
&nbsp; 
Another application of cluster analysis is to identify regions with similar
weathers by using info such as latitude/longitude, humidity, temperature, sea level, altitude, etc.  
&nbsp; 
Lastly, another application of cluster analysis could be to group together students within a school based on criterion such as age, ethnicity, family income, GPA, etc.  

\newpage

## Question 5

5. What are the advantages and disadvantages of a very flexible (versus a less
flexible) approach for regression or classification? Under what circumstances
might a more flexible approach be preferred to a less flexible approach? When
might a less flexible approach be preferred?  

_Solution:_

The advantage of a very flexible model is that the model will have low bias because it can model the data precisely. Another advantage of flexible models is that they can model more complex relationship between variables. Undoubtedly, any real-world relationships are not constant or linear.  
&nbsp;  
A disadvantage of a very flexible model is the large number of parameters that need to be estimated. This makes the model less interpretable. This problem is
exaceberated if we have a low number of observations but a high number of
predictor variables. Another disadvantage of a flexible model is that it will 
most likely have high variance. The model will have a low train MSE, but will have a high test MSE since the model will struggle to accurately predict given brand new observations. The model was taught to adhere too closely to the training data which causes it to struggle with new test data.  
&nbsp;  
A more flexible approach is preferred if the main goal is accurate prediction. Perhaps, accurate prediction would help a company's bottom line or provide a critical diagonosis on a patient.  
&nbsp;  
On the other hand, a less flexible approach is preferred if the main goal is inference. That is, we value interprebility and seek to understand the relationship between variables.  

\newpage

## Question 6

6. Describe the differences between a parametric and a non-parametric
statistical learning approach. What are the advantages of a parametric approach
to regression or classification (as opposed to a nonparametric approach)? What
are its disadvantages?

_Solution:_  
Parametric statistical methods make assumptions about the functional form of f and reduce the problem of estimating f to solving for a set of parameters For example, linear regression is a parametric method which assumes that f is linear in its predictors X. To implement linear regression, one can use least squares to solve for $\beta$.  
&nbsp;  
Nonparametric statistical methods do not make assumptions about the functional form of f. Instead, they attempt to estimate f through mathematical techniques.  
&nbsp;  
An advantage of a parametric method is that it is easy to estimate a set of parameters compared to estimating an entire function for nonparametric methods.  
&nbsp;  
A disadvantage of a parametric method is that the true form of f may be completely different than what the stated assumptions of the parametic method. This is where nonparametric methods shine as they can estimate a greater variety of functional forms of f. However, because of the high flexibility of nonparametric methods, these methods can result in high variance and overfitting. Nonparametrics methods require a very large number of observations to produce accurate estimate because they cannot reduce problem to estimating a small set of coefficients.

\newpage

## Question 7

7. The table below provides a training data set containing six observations, three predictors, and one qualitative response variable.

Suppose we wish to use this data set to make a prediction for Y when
X1 = X2 = X3 = 0 using K-nearest neighbors.
(a) Compute the Euclidean distance between each observation and the test point,
X1 = X2 = X3 = 0.
(b) What is our prediction with K = 1? Why?
(c) What is our prediction with K = 3? Why?
(d) If the Bayes decision boundary in this problem is highly nonlinear, then
would we expect the best value for K to be large or small? Why?

_Solution:_

The dataset is recreated below.
```{r}
df <- data.frame(X1 = c(0, 2, 0, 0, -1, 1), X2 = c(3, 0, 1, 1, 0, 1),
                 X3 = c(0, 0, 3, 2, 1, 1),
                 Y = c("Red", "Red", "Red", "Green", "Green", "Red"))

# Test Point
TestPoint <- c(0, 0, 0)

df <- rbind(df, TestPoint)

# calculate euclidean distance
# last row is eucliden distance of test point to all observations
# in training set
dist(df[1:7, 1:3], method = "euclidean")

```

(b) When K = 1, our predicition is Green. Because the closest observation to the test point is observation 5 and it is Green.

(c) When K = 3, our prediction is Green. The three closest observations
to the test point are observations 4, 5, and 6. Observations 4 and 5 are Green and observation 6 is Red. So the estimated probability for the Green Class is 2/3 and K-Nearest Neighbors predicts that the test point belongs to the Green class.

# Applied Exercises

\newpage

## Question 8

_Solution:_

(a) Load data
```{r}
library(here)
college <- read.csv(here("Chapter2-StatisticalLearning", "College.csv"))
```

(b) View data

```{r, eval = FALSE}
# opens a data editor
rownames(college) <- college[ , 1]
fix(college)
college <- college[ ,-1]
fix(college)
```

(c)

i. Summary of all the variabes 
```{r}
summary(college)
```


ii. Scatterplot matrix
```{r}
pairs(college[, 1:10])
```


iii. Boxplot of `Outstate` vs `Private`
```{r}
plot(college$Private, college$Outstate,
     xlab = "Private School",
     ylab = "Out-of-state tuition")
```


iv.

After creating the new `Elite` column, we find that
there are 78 elite universites. 
```{r}
# create vector of "No" with length of the dataset
Elite <- rep ("No",nrow(college ))

# University is elite if the top 10% of the high
# school class exceeds 50%
Elite[college$Top10perc > 50] <- " Yes"

# coerce to factor
Elite = as.factor(Elite)

# bind to data frame
college <- data.frame(college, Elite)

summary(college$Elite)

```

By plotting elite status against out-of state tuition,
we find that elite universities tend to have higher
out-of-state tuition costs. 
```{r}
plot(college$Elite, college$Outstate,
     xlab = "Elite University",
     ylab = "Out-of-state tuition")
```


(v) Some histograms of the quantitative variables

```{r}
par(mfrow=c(2,2))
hist(college$Outstate, xlab = "Out-of-state tuition", main = "")
hist(college$S.F.Ratio, xlab = "Student-Faculty Ratio", main = "")
hist(college$Grad.Rate, xlab = "Graduation Rate", main = "")
hist(college$Expend, xlab = "Insructional expenditure per student", main = "", breaks = 20)

```


\newpage

## Question 9

_Solution:_
(a) The quantitative varibles are: `mpg`, `cylinders`,
`displacement`, `horsepower`, `weight`, `acceleration`, `year`, and `origin` The sole qualitiative variables is `name`
```{r}
auto <- ISLR::Auto
# determine quantiative variables
sapply(auto, is.numeric)
```

(b) Range of numerical variables
```{r}
# auto data with only numeric columns
auto_numeric <- auto[, purrr::map_lgl(auto, is.numeric)]
t(sapply(auto_numeric, range))
```

(c) Mean and standard deviation of numerical variables
```{r}
sapply(auto_numeric, mean)
sapply(auto_numeric, sd)
```

(d) Remove some observations. Recompute mean and sd.
```{r}
auto_numeric_sparse <- auto_numeric[-c(10:85),]
t(sapply(auto_numeric_sparse, range))
sapply(auto_numeric_sparse, mean)
sapply(auto_numeric_sparse, sd)
```


(e) Exploratory analysis
```{r}
par(mfrow=c(2,3))
plot(auto$acceleration, auto$mpg, 
     xlab = "acceleration", 
     ylab = "mpg")
plot(auto$year, auto$mpg,
     xlab = "Year", 
     ylab = "mpg")
plot(auto$horsepower, auto$mpg,
     xlab = "horsepower", 
     ylab = "mpg")
plot(auto$cylinders, auto$mpg,
     xlab = "cylinders", 
     ylab = "mpg")
plot(auto$weight, auto$mpg,
     xlab = "weight", 
     ylab = "mpg")
plot(auto$displacement, auto$mpg,
     xlab = "displacement", 
     ylab = "mpg")

```

(f) Predicting mpg

The above scatterplots suggest that `weight`, `horsepower`, and `weight` might be useful to predict `mpg.` It appears that there is an inverse relationship between the variables and `mpg.` For example, a heavier car will have lower mpg. There also appears to be a positive relationship between the year of the car and mpg. The newer the make of the car, the better mpg it has. For the `cylinders` and `acceleration` variables, the relationship to `mpg` is more ambiguous. It apperas that higher acceleration means the car also has high mpg, but up to an extent. The acceleration vs mpg plots suggests a diminishing returns in acceleration to mpg. Lastly, for any number of cylinders, the range for mpg varies across cars. 


\newpage

## Question 10

_Solution:_

(a) Load data.


```{r}
library(MASS)
boston <- Boston
head(boston)
```

The `boston` dataset has data on housing values in the suburbs of boston. There
are `r nrow(boston)` rows and `r ncol(boston)` columns. Each row represents one suburban towns in boston. The columns represent different attributes about the town. For example, there is data on the crime rate,  median values of owner-occupied homes, and an index of accessibility to radial highways.

(b) Scatterplots


Some observations from the scatterplots. It appears that towns with a higher per capita crime rate (`crim`) are farther from boston employment centers (`dis`). Also, there is a positive relationship between the average number of rooms per dwelling (`rm`) and the median value of owner-occupied homes (`medv`). THere also appears to be greater range for the percentage of lower status in the population (`lstat`) given a higher proportion of units built prior to 1940 (`age`)
```{r}
pairs(boston[, c("crim", "rm", "dis", "black", "medv")])

pairs(boston[, c("crim", "nox", "rad", "indus", "ptratio")])

pairs(boston[, c("crim", "zn", "chas", "age", "tax", "lstat")])
```

(c) Which variables have a relationship to per capita crime rate.

The scatterplot suggest a positive relationship between weighted mean distance from major employment centers and per capita crime rates. Also, there is a mild positive relationship between percentage of lower status of the population and proportion of owner-occupied units built prior to 1940 with per capita crime rate.

(d) Ranges of quantitative variables

The per capita crime rate varies from `r range(boston$crim)[1]` to `r range(boston$crim)[2]`. The tax rate varies from `r range(boston$tax)[1]` to `r range(boston$tax)[2]` And, The tax rate varies from `r range(boston$ptratio)[1]` to `r range(boston$ptratio)[2]`. There is a wide range for the per capita crime rate and the tax rate. The range for the pupil-teacher ratios is not as wide.


(e) How many of the suburbs bound the Charles River?

`r sum(boston$chas == 1)` Boston suburbs bound the Charles River

(f) Median pupil teacher ratio among towns

The median pupil teach ratio among towns is `r median(boston$ptratio)`

(g) 

The lowest median value of owner occupied home is `r min(boston$medv)`. This value corresponds to suburb 399 and 406.


The values for the other predictor for suburb 399 is shown below
```{r}
t(boston[399, ])
```


The values for the other predictors for suburb 406 is shown below
```{r}
t(boston[406, ])
```

Below is the overall range for the predictors.
```{r}
t(sapply(boston, range))
```

The predictor values for suburb 406 had crime rates closer to the upper bound for `crim`. Both suburbs had the maximum value for `age`, the proportion of owner-occupied units built prior to 1940. Both suburbs were close to Boston employment centers (`dis`), compared to the range of `dis`. Also, the value for `black` and `tax` was much closer to the upper bound for the range of `black` and `tax`. Also, these suburbs had a high pupil-teacher ratio compared to the range of `ptratio`. And, the percentage of the lower status of the population skewed toward the upper bound of the range for `lstat`.

(h) 

There are `r sum(boston$rm > 7)` suburbs that average more than 7 rooms per dwelling. And `r sum(boston$rm > 8)` suburbs that average more than 8 rooms per dwelling.


Below are the suburbs that average more than 8 rooms per dwelling.
```{r}
boston[boston$rm > 8, ]
```

It appears that most of these suburbs are not bound by the Charles river, have a high proportion of blacks, have a high proprtion of owner-occupied units built prior to 1940, have a lower percentage of lower status of the population, have a high medican value of owner-occupied homes, have a low crime rate, and are closer to Boston employment centres.

\newpage
